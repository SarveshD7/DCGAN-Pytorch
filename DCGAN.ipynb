{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH0Nq8bz4Whg9dBOqvPxwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarveshD7/DCGAN-Pytorch/blob/main/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXVjEkIV2fvh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Important Points on the Architecture of Official Implementation of DCGAN***\n",
        "The Generator and Discriminator use the same set of Blocks but just ar the opposites of each other.\n",
        "\n",
        "The Generator uses Transpose Convolutions for UpScaling the image, whereas Discriminator uses normal Convolutions.\n",
        "\n",
        "Official Implementation of DCGAN does not use BatchNorm in only the First Layer of Discriminaotor and Last Layer of Generator.\n",
        "\n",
        "So We need to specifically write these layers and use _block for rest.\n",
        "\n",
        "Since we are using BatchNorm in all other layers we do bias=False for them.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LZhGDopMDtw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, channels_img, features_d):  # features_d is channels that are going to change in different layers\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.disc = nn.Sequential(\n",
        "        # Input: N x channels_img x 64 x 64\n",
        "        nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),  # 32x32\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self._block(features_d, features_d*2, 4,2,1),  # 16x16\n",
        "        self._block(features_d*2, features_d*4, 4,2,1),  # 8x8\n",
        "        self._block(features_d*4, features_d*8, 4,2,1),  # 4x4\n",
        "        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), # 1x1\n",
        "        nn.Sigmoid()  # Finally we get a single value telling us whether the image passed is real or fake\n",
        "    )\n",
        "\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        # nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.disc(x)\n"
      ],
      "metadata": {
        "id": "T-5GQFokByJz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, channels_img, features_g):\n",
        "    super(Generator, self).__init__()\n",
        "    # Input: N x z_dim x 1 x 1\n",
        "    self.gen = nn.Sequential(\n",
        "        self._block(z_dim, features_g*16, 4, 1, 0), # 4x4\n",
        "        self._block(features_g*16, features_g*8, 4, 2, 1), # 8x8\n",
        "        self._block(features_g*8, features_g*4, 4, 2, 1), # 16x16\n",
        "        self._block(features_g*4, features_g*2, 4, 2, 1), # 32x32\n",
        "        nn.ConvTranspose2d(features_g*2,channels_img, kernel_size=4, stride=2, padding=1),  # 64x64\n",
        "        nn.Tanh()  # Range = [-1,1]\n",
        "    )\n",
        "\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        # nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.gen(x)\n"
      ],
      "metadata": {
        "id": "3-TjScMsF8UN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing weights of the model with a normal distribution with given mean and standard deviation\n",
        "def initialize_weights(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "      nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ],
      "metadata": {
        "id": "-T62_RB1JrlV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  N, in_channels, H, W = 8, 3, 64, 64\n",
        "  z_dim = 100\n",
        "  x = torch.randn((N, in_channels, H, W))\n",
        "  disc = Discriminator(in_channels, 8)\n",
        "  initialize_weights(disc)\n",
        "  assert disc(x).shape == (N, 1, 1, 1)\n",
        "  gen = Generator(z_dim, in_channels, 8)\n",
        "  initialize_weights(gen)\n",
        "  z = torch.randn((N, z_dim, 1, 1))\n",
        "  assert gen(z).shape == (N, in_channels, H, W)\n",
        "  print(\"Success\")"
      ],
      "metadata": {
        "id": "h3YnddJ8UtTs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RJ-f_01VGNS",
        "outputId": "26456261-5b90-4571-aa40-11f71754d609"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 1\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64"
      ],
      "metadata": {
        "id": "8M4TgdTAVLSU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = torchvision.transforms.Compose(\n",
        "   [ torchvision.transforms.Resize(IMAGE_SIZE),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)],\n",
        "    )]\n",
        ")"
      ],
      "metadata": {
        "id": "v8ZRmpDQ5ZCh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST(root=\"/content/dataset/\", train=True, transform=transforms, download=True)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "KAZHiKy_52DH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)"
      ],
      "metadata": {
        "id": "xqQ7-Nf67tt1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "dBbMAztb708Z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_noise = torch.randn(32, Z_DIM,1,1).to(device)"
      ],
      "metadata": {
        "id": "txGFu38B8We_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI0nuLL08o65",
        "outputId": "636e8925-d8b6-430f-ff13-d69e35631254"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (gen): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (4): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (5): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disc.train()"
      ],
      "metadata": {
        "id": "addN4hG38rJW",
        "outputId": "e1cff931-4f89-42fe-a92a-bef69d371984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (disc): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (6): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrting the training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"Started epoch-> {epoch}\")\n",
        "  for batch_idx, (real, _) in enumerate(loader):\n",
        "    real = real.to(device)\n",
        "    noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
        "    fake = gen(noise)\n",
        "    # Train Discriminator--> Maximize log(D(x)) + log(1-D(G(z)))\n",
        "    disc_real = disc(real).reshape(-1)\n",
        "    loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "    # criterion-> BCE Loss-> ℓ(x,y) = -wn*[yn . logxn +(1-yn) . log(1-xn)] -> We are passing y as all ones so ulitmately this term will become the first component of the Discriminator Loss\n",
        "    disc_fake = disc(fake).reshape(-1)\n",
        "    loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "    # criterion-> BCE Loss-> ℓ(x,y) = -wn*[yn . logxn +(1-yn) . log(1-xn)] -> We are passing y as all zeros so ulitmately this term will become the second component of the Discriminator Loss\n",
        "    loss_disc = (loss_disc_real + loss_disc_fake)/2\n",
        "    disc.zero_grad()\n",
        "    loss_disc.backward(retain_graph=True)  # retain_graph=True since we want to reuse fake and Pytorch erases intermediate results on loss.backward()\n",
        "    opt_disc.step()\n",
        "\n",
        "    # Training the Generator---> Minimize log(1-D(G(z)) <----> Maximize log(D(g(z)))\n",
        "    output = disc(fake).reshape(-1)\n",
        "    loss_gen = criterion(output, torch.ones_like(output))\n",
        "    gen.zero_grad()\n",
        "    loss_gen.backward()\n",
        "    opt_gen.step()\n",
        "  print(f\"Completed epoch-> {epoch}\")\n"
      ],
      "metadata": {
        "id": "4Pfw7o6J8vQj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "e4b2ddd2-ecfe-4041-a399-447d205c043c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started epoch-> 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-be3752f8d986>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mopt_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Completed epoch-> {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GdREiKAAkux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}